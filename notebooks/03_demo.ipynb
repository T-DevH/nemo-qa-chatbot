{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Demo for NeMo QA Chatbot\n",
    "\n",
    "This notebook provides an interactive demo of the NeMo QA Chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import NeMo QA modules\n",
    "from nemo_qa.modeling.model import load_model\n",
    "from nemo_qa.recipes.inference_recipe import format_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Let's load the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Replace with your model path\n",
    "model_path = '../models/finetuned/final_model'\n",
    "\n",
    "# Load model\n",
    "model = load_model(model_path)\n",
    "\n",
    "print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Chat Function\n",
    "\n",
    "Let's create a chat function to interact with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def chat(question, context=\"\", history=None, temperature=0.7):\n",
    "    \"\"\"Chat with the model.\n",
    "    \n",
    "    Args:\n",
    "        question: User question\n",
    "        context: Optional context\n",
    "        history: Conversation history\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Model response\n",
    "    \"\"\"\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = format_prompt(question, context, history)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            return_attention=True\n",
    "        )\n",
    "    \n",
    "    # Extract response\n",
    "    response = output['text'][0].replace(prompt, \"\").strip()\n",
    "    attention = output.get('attention_weights')\n",
    "    \n",
    "    return response, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chat Function\n",
    "\n",
    "Let's test the chat function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test with a simple question\n",
    "question = \"What is LoRA fine-tuning?\"\n",
    "response, _ = chat(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gradio Interface\n",
    "\n",
    "Now let's create a Gradio interface for the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def gradio_chat(message, history, context):\n",
    "    \"\"\"Chat function for Gradio interface.\n",
    "    \n",
    "    Args:\n",
    "        message: User message\n",
    "        history: Conversation history\n",
    "        context: Optional context\n",
    "    \n",
    "    Returns:\n",
    "        Updated history\n",
    "    \"\"\"\n",
    "    # Convert Gradio history to the format expected by the chat function\n",
    "    chat_history = [(h[0], h[1]) for h in history]\n",
    "    \n",
    "    # Chat with the model\n",
    "    response, _ = chat(message, context, chat_history)\n",
    "    \n",
    "    # Return the updated history\n",
    "    return response\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"NeMo QA Chatbot\") as demo:\n",
    "    gr.Markdown(\"# NeMo QA Chatbot Demo\")\n",
    "    gr.Markdown(\"Ask questions about LoRA fine-tuning, NeMo, and related topics.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(height=600)\n",
    "            \n",
    "            with gr.Row():\n",
    "                message = gr.Textbox(\n",
    "                    label=\"Question\",\n",
    "                    placeholder=\"Type your question here...\",\n",
    "                    show_label=False\n",
    "                )\n",
    "                submit_btn = gr.Button(\"Send\")\n",
    "        \n",
    "        with gr.Column(scale=2):\n",
    "            context = gr.Textbox(\n",
    "                label=\"Context (Optional)\",\n",
    "                placeholder=\"Add optional context here...\",\n",
    "                lines=10\n",
    "            )\n",
    "    \n",
    "    gr.Markdown(\"## Example Questions\")\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            \"What is LoRA fine-tuning?\",\n",
    "            \"How does NeMo Curator improve data quality?\",\n",
    "            \"Explain the advantages of LLAMA3 over LLAMA2.\",\n",
    "            \"What are the key hyperparameters for LoRA?\",\n",
    "            \"How can I implement efficient LoRA fine-tuning in NeMo?\"\n",
    "        ],\n",
    "        inputs=message\n",
    "    )\n",
    "    \n",
    "    # Set up event handlers\n",
    "    submit_btn.click(\n",
    "        gradio_chat,\n",
    "        inputs=[message, chatbot, context],\n",
    "        outputs=chatbot\n",
    "    )\n",
    "    \n",
    "    message.submit(\n",
    "        gradio_chat,\n",
    "        inputs=[message, chatbot, context],\n",
    "        outputs=chatbot\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Testing\n",
    "\n",
    "Let's test the model with some more advanced scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test with context\n",
    "question = \"What are the key hyperparameters?\"\n",
    "context = \"\"\"\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that uses low-rank matrices\n",
    "to adapt pre-trained models. The key hyperparameters in LoRA are:\n",
    "- Rank (r): Controls the rank of the low-rank matrices\n",
    "- Alpha (Î±): Scaling factor for LoRA updates\n",
    "- Target modules: Which layers to apply LoRA to\n",
    "- Dropout: Regularization parameter\n",
    "\"\"\"\n",
    "\n",
    "response, _ = chat(question, context)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test with conversation history\n",
    "history = [\n",
    "    (\"What is NeMo?\", \"NeMo (Neural Modules) is NVIDIA's framework for building, training, and fine-tuning neural networks for various AI tasks, including natural language processing.\")\n",
    "]\n",
    "\n",
    "question = \"How does it support LoRA fine-tuning?\"\n",
    "response, _ = chat(question, history=history)\n",
    "\n",
    "print(f\"History: {history}\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
