{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis for NeMo QA Chatbot\n",
    "\n",
    "This notebook analyzes the fine-tuned model for the NeMo QA Chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import NeMo QA modules\n",
    "from nemo_qa.modeling.model import load_model\n",
    "from nemo_qa.modeling.evaluation import evaluate_model, compute_exact_match, compute_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Let's load the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Replace with your model path\n",
    "model_path = '../models/finetuned/final_model'\n",
    "\n",
    "# Load model\n",
    "model = load_model(model_path)\n",
    "\n",
    "print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data\n",
    "\n",
    "Let's load the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Replace with your test data path\n",
    "test_data_path = '../data/datasets/test.jsonl'\n",
    "\n",
    "# Load test data\n",
    "test_data = []\n",
    "with open(test_data_path, 'r') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Now let's evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Limit number of samples for faster evaluation\n",
    "max_samples = 10\n",
    "eval_data = test_data[:max_samples]\n",
    "\n",
    "# Evaluate model\n",
    "metrics = evaluate_model(\n",
    "    model=model.model,\n",
    "    test_data=eval_data,\n",
    "    output_path='../evaluation_results.json',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Exact Match: {metrics['exact_match']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"Relevance Score: {metrics['relevance_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Model Outputs\n",
    "\n",
    "Let's analyze the model outputs in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load evaluation results\n",
    "with open('../evaluation_results.json', 'r') as f:\n",
    "    eval_results = json.load(f)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(eval_results['results'])\n",
    "\n",
    "# Add length columns\n",
    "results_df['question_length'] = results_df['question'].apply(len)\n",
    "results_df['reference_length'] = results_df['reference'].apply(len)\n",
    "results_df['prediction_length'] = results_df['prediction'].apply(len)\n",
    "\n",
    "# Plot score distributions\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.histplot(results_df['exact_match'], bins=2, ax=ax[0])\n",
    "ax[0].set_title('Exact Match Distribution')\n",
    "ax[0].set_xlabel('Exact Match')\n",
    "ax[0].set_ylabel('Count')\n",
    "\n",
    "sns.histplot(results_df['f1_score'], bins=10, ax=ax[1])\n",
    "ax[1].set_title('F1 Score Distribution')\n",
    "ax[1].set_xlabel('F1 Score')\n",
    "ax[1].set_ylabel('Count')\n",
    "\n",
    "sns.histplot(results_df['relevance_score'], bins=10, ax=ax[2])\n",
    "ax[2].set_title('Relevance Score Distribution')\n",
    "ax[2].set_xlabel('Relevance Score')\n",
    "ax[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Predictions\n",
    "\n",
    "Let's look at some example predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort by F1 score\n",
    "results_df = results_df.sort_values('f1_score', ascending=False)\n",
    "\n",
    "# Print best predictions\n",
    "print(\"Best predictions:\")\n",
    "for i, row in results_df.head(3).iterrows():\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    print(f\"F1 Score: {row['f1_score']:.4f}\")\n",
    "    print(\"---\\n\")\n",
    "\n",
    "# Print worst predictions\n",
    "print(\"Worst predictions:\")\n",
    "for i, row in results_df.tail(3).iterrows():\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    print(f\"F1 Score: {row['f1_score']:.4f}\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing\n",
    "\n",
    "Let's test the model interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(question, temperature=0.7):\n",
    "    \"\"\"Generate response from the model.\"\"\"\n",
    "    prompt = f\"Human: {question}\\nAssistant:\"\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            top_k=50\n",
    "        )\n",
    "    \n",
    "    # Extract response\n",
    "    response = output['text'][0].replace(prompt, \"\").strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test with a custom question\n",
    "question = \"What is LoRA fine-tuning?\"\n",
    "response = generate_response(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Model Performance by Question Type\n",
    "\n",
    "Let's analyze the model performance by question type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Categorize questions by starting word\n",
    "def get_question_type(question):\n",
    "    \"\"\"Get question type based on starting word.\"\"\"\n",
    "    question = question.strip().lower()\n",
    "    \n",
    "    if question.startswith('what'):\n",
    "        return 'What'\n",
    "    elif question.startswith('how'):\n",
    "        return 'How'\n",
    "    elif question.startswith('why'):\n",
    "        return 'Why'\n",
    "    elif question.startswith('when'):\n",
    "        return 'When'\n",
    "    elif question.startswith('where'):\n",
    "        return 'Where'\n",
    "    elif question.startswith('who'):\n",
    "        return 'Who'\n",
    "    elif question.startswith('which'):\n",
    "        return 'Which'\n",
    "    elif question.startswith('can') or question.startswith('do') or question.startswith('is') or question.startswith('are'):\n",
    "        return 'Yes/No'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Add question type column\n",
    "results_df['question_type'] = results_df['question'].apply(get_question_type)\n",
    "\n",
    "# Group by question type\n",
    "question_type_metrics = results_df.groupby('question_type').agg({\n",
    "    'exact_match': 'mean',\n",
    "    'f1_score': 'mean',\n",
    "    'relevance_score': 'mean',\n",
    "    'question': 'count'\n",
    "}).rename(columns={'question': 'count'}).reset_index()\n",
    "\n",
    "# Plot performance by question type\n",
    "plt.figure(figsize=(12, 8))\n",
    "question_type_metrics = question_type_metrics.sort_values('count', ascending=False)\n",
    "\n",
    "x = np.arange(len(question_type_metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.bar(x - width, question_type_metrics['exact_match'], width, label='Exact Match')\n",
    "ax.bar(x, question_type_metrics['f1_score'], width, label='F1 Score')\n",
    "ax.bar(x + width, question_type_metrics['relevance_score'], width, label='Relevance Score')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(question_type_metrics['question_type'])\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance by Question Type')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print question type statistics\n",
    "print(question_type_metrics.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
